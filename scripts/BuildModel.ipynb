{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축\n",
    "\n",
    "1. AI-Hub 데이터로 감정분석 모델을 먼저 학습\n",
    "2. 크롤링 등의 방법으로 관광 명소 리뷰로 **도메인 전이 학습**\n",
    "3. 관광 리뷰와 관련된 키워드를 강화해 최종 모델 구축\n",
    "\n",
    "\n",
    "2. 학습 데이터 -> AI-Hub 데이터 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Hub 데이터로 감정분석 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_len = 512\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 데이터 로드\n",
    "일단 하나만 불러옴\n",
    "\n",
    "TODO : 모든 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "proj_root = Path('.').resolve().parent\n",
    "data_path = os.path.join(proj_root, 'data', 'json')\n",
    "\n",
    "json_files = glob.glob(data_path + '/TL_SNS_*/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. 데이터셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# DataSet Class\n",
    "\n",
    "relevant_aspects = ['가격', '서비스', '품질']\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, json_files, tokenizer, max_len):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for file in json_files:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for review in data:\n",
    "                    sentence = review[\"RawText\"]\n",
    "                    for aspect in review[\"Aspects\"]:\n",
    "                        aspect_term = aspect[\"Aspect\"]\n",
    "                        if aspect_term in relevant_aspects:\n",
    "                            sentiment = int(aspect[\"SentimentPolarity\"]) + 1 # 0: Negative, 1: Neutral, 2: Positive\n",
    "                            combined_input = f\"{sentence} [SEP] {aspect_term}\"\n",
    "                            self.sentences.append(combined_input)\n",
    "                            self.labels.append(sentiment)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        combined_input = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            combined_input,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 데이터 전처리(Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT에는 Tokenizer를 통해 입력데이터를 토큰화하여 모델에 입력함.\n",
    "\n",
    "\n",
    "BERT모델은 일반적으로 다음을 입력으로 받음\n",
    "1. input_ids: 토큰화된 입력 데이터의 ID\n",
    "2. attention_mask: 유효한 토큰과 패딩 토큰을 구분하는 마스크(1: 유효한 토큰, 0: 패딩 토큰)\n",
    "3. token_type_ids: 문장이 하나인지, 두개인지 구분하는 ID(0: 첫번째 문장, 1: 두번째 문장, 기본값은 0)\n",
    "\n",
    "tokenizer는 Huggingface transformer에서 제공하는 라이브러리중에서 사용함. : skt/kobert-base-v1\n",
    "\n",
    "tokenizer\n",
    "1. texts\n",
    "2. truncation=True : 길이가 max_length를 넘어가면 잘라냄\n",
    "3. padding=True: 길이가 부족한 경우 패딩 추가\n",
    "4. max_length=512: 최대 길이 설정\n",
    "5. return_tensors=\"pt\": 파이토치 텐서로 반환\n",
    "6. return_token_type_ids=True: 토큰 타입 아이디 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\", sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "\n",
    "train_dataset = ABSADataset(json_files, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 모델 로드\n",
    "Pytorch에서 데이터를 효율적으로 처리하고 학습에 필요한 배치 단위로 데이터를 공급\n",
    "데이터를 학습에 사용할 수 있는 형태로 정리, 배치 단위로 나누거나 섞는 작업을 자동화함\n",
    "\n",
    "Batch\n",
    "학습 중 한 번에 모델에 입력되는 데이터의 개수\n",
    "모든 데이터를 한번에 처리하지 않고, 일정한 크기로 나눠서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 옵티마이저, 손실함수 정의\n",
    "- 옵티마이저 : 모델의 파라미터를 업데이트 하는 알고리즘\n",
    "    - 목표 : 손실함수 값을 최소화 하는 방향으로 모델 파라미터 조정\n",
    "    - adamW 사용 Adam에서 가중치 감소를 추가하여 과적합을 방지함\n",
    "- 손실함수 : 모델의 출력(예측값)과 실제 정답(레이블)간의 차이를 수치적으로 나타낸 것\n",
    "    - 해당 값을 기반으로 옵티마이저가 가중치를 업데이트함\n",
    "    - BERT에서는 기본으로 Pytorch의 CrossEntropyLoss사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Secheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast(\"cuda:0\"):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"kobert-finetuned\")\n",
    "tokenizer.save_pretrained(\"kobert-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 예시\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_sentence = \"이 제품은 디자인이 정말 멋져요\"\n",
    "    test_aspect = \"디자인\"\n",
    "    combined_input = f\"{test_sentence} [SEP] {test_aspect}\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        combined_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    sentiment_map = {0: \"부정\", 1: \"중립\", 2: \"긍정\"}\n",
    "    print(f\"Aspect '{test_aspect}'에 대한 감성 분석 결과: {sentiment_map[prediction]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "013a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
