{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축\n",
    "\n",
    "1. AI-Hub 데이터로 감정분석 모델을 먼저 학습\n",
    "2. 크롤링 등의 방법으로 관광 명소 리뷰로 **도메인 전이 학습**\n",
    "3. 관광 리뷰와 관련된 키워드를 강화해 최종 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Hub 데이터로 감정분석 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_len = 512\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "proj_root = Path('.').resolve().parent\n",
    "data_path = os.path.join(proj_root, 'data', 'json')\n",
    "\n",
    "json_files = glob.glob(data_path + '/TL_SNS_*/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 데이터셋 클래스 생성\n",
    "\n",
    "> `relevant_aspects`에 임의로 학습시킬 키워드 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# DataSet Class\n",
    "\n",
    "relevant_aspects = ['가격', '서비스', '품질']\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, json_files, tokenizer, max_len):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for file in json_files:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for review in data:\n",
    "                    sentence = review[\"RawText\"]\n",
    "                    for aspect in review[\"Aspects\"]:\n",
    "                        aspect_term = aspect[\"Aspect\"]\n",
    "                        if aspect_term in relevant_aspects:\n",
    "                            sentiment = int(aspect[\"SentimentPolarity\"]) + 1 # 0: Negative, 1: Neutral, 2: Positive\n",
    "                            combined_input = f\"{sentence} [SEP] {aspect_term}\"\n",
    "                            self.sentences.append(combined_input)\n",
    "                            self.labels.append(sentiment)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        combined_input = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            combined_input,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 데이터 전처리(Tokenizer)\n",
    "tokenizer는 Huggingface transformer에서 제공하는 라이브러리중에서 사용함. : skt/kobert-base-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\", sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "\n",
    "train_dataset = ABSADataset(json_files, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 모델 로드\n",
    "skt/kobert-base-v1 기반으로, SequenceClassification모델 사용, label은 3개(긍정, 중립, 부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 옵티마이저\n",
    "- 옵티마이저: adamW 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Secheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 학습 루프\n",
    "epoch = 3\n",
    "batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast(\"cuda:0\"):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"kobert-finetuned\")\n",
    "tokenizer.save_pretrained(\"kobert-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 평가\n",
    "\n",
    "Todo : validation 셋으로 모델 성능 평가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 예시\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_sentence = \"이 제품은 디자인이 정말 멋져요\"\n",
    "    test_aspect = \"디자인\"\n",
    "    combined_input = f\"{test_sentence} [SEP] {test_aspect}\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        combined_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    sentiment_map = {0: \"부정\", 1: \"중립\", 2: \"긍정\"}\n",
    "    print(f\"Aspect '{test_aspect}'에 대한 감성 분석 결과: {sentiment_map[prediction]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "013a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
